{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d350a92",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b681e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import mlflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472cbec5",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93dd2253",
   "metadata": {},
   "source": [
    "##### Loading train and inference files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c61ee90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I caught this little gem totally by accident b...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I can't believe that I let myself into this mo...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>*spoiler alert!* it just gets to me the nerve ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>If there's one thing I've learnt from watching...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I remember when this was in theaters, reviews ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39995</th>\n",
       "      <td>It's the 1920s. And a man named Walt Disney wa...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39996</th>\n",
       "      <td>The first (and only) time I saw \"Shades\" was d...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39997</th>\n",
       "      <td>This was such a waste of time. Danger: If you ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39998</th>\n",
       "      <td>this is by far the most pathetic movie Indian ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39999</th>\n",
       "      <td>This movie forever left an impression on me. I...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review sentiment\n",
       "0      I caught this little gem totally by accident b...  positive\n",
       "1      I can't believe that I let myself into this mo...  negative\n",
       "2      *spoiler alert!* it just gets to me the nerve ...  negative\n",
       "3      If there's one thing I've learnt from watching...  negative\n",
       "4      I remember when this was in theaters, reviews ...  negative\n",
       "...                                                  ...       ...\n",
       "39995  It's the 1920s. And a man named Walt Disney wa...  positive\n",
       "39996  The first (and only) time I saw \"Shades\" was d...  negative\n",
       "39997  This was such a waste of time. Danger: If you ...  negative\n",
       "39998  this is by far the most pathetic movie Indian ...  negative\n",
       "39999  This movie forever left an impression on me. I...  negative\n",
       "\n",
       "[40000 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/raw/train.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7f96bef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Yes, MTV there really is a way to market Daria...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The story of the bride fair is an amusing and ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A team varied between Scully and Mulder, two o...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This was a popular movie probably because of t...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This movie made me so angry!! Here I am thinki...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>SILVER CITY (2+ outta 5 stars) As a huge fan o...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>Moscow Zero stole my money and I want it back!...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>This is the only film I've seen that is made b...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>This is a story about Shin-ae, who moves to Mi...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>Leon Errol handles his double role of Uncle Ma...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 review sentiment\n",
       "0     Yes, MTV there really is a way to market Daria...  negative\n",
       "1     The story of the bride fair is an amusing and ...  negative\n",
       "2     A team varied between Scully and Mulder, two o...  positive\n",
       "3     This was a popular movie probably because of t...  negative\n",
       "4     This movie made me so angry!! Here I am thinki...  negative\n",
       "...                                                 ...       ...\n",
       "9995  SILVER CITY (2+ outta 5 stars) As a huge fan o...  negative\n",
       "9996  Moscow Zero stole my money and I want it back!...  negative\n",
       "9997  This is the only film I've seen that is made b...  negative\n",
       "9998  This is a story about Shin-ae, who moves to Mi...  positive\n",
       "9999  Leon Errol handles his double role of Uncle Ma...  negative\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_inf = pd.read_csv(\"../data/raw/inference.csv\")\n",
    "df_inf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92972b6",
   "metadata": {},
   "source": [
    "##### Checking data imbalance in target column (everything is perfect in each datasets target is 50/50 distibuted, which is amazing for training and evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8caefdfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "positive    20000\n",
       "negative    20000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cda5a6a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "negative    5000\n",
       "positive    5000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_inf.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de4b640",
   "metadata": {},
   "source": [
    "##### Also data is free from missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c25ebaa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "review       0\n",
       "sentiment    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e96c7314",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "review       0\n",
       "sentiment    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_inf.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db46d09",
   "metadata": {},
   "source": [
    "##### But surprisingly there are some duplicates in each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e31a2f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(272)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed55c803",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(13)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_inf.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee00fa6",
   "metadata": {},
   "source": [
    "##### Delete duplicates and convert target to number (1,0) - int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29bd343b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Yes, MTV there really is a way to market Daria...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The story of the bride fair is an amusing and ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A team varied between Scully and Mulder, two o...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This was a popular movie probably because of t...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This movie made me so angry!! Here I am thinki...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>SILVER CITY (2+ outta 5 stars) As a huge fan o...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>Moscow Zero stole my money and I want it back!...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>This is the only film I've seen that is made b...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>This is a story about Shin-ae, who moves to Mi...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>Leon Errol handles his double role of Uncle Ma...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9987 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 review sentiment\n",
       "0     Yes, MTV there really is a way to market Daria...  negative\n",
       "1     The story of the bride fair is an amusing and ...  negative\n",
       "2     A team varied between Scully and Mulder, two o...  positive\n",
       "3     This was a popular movie probably because of t...  negative\n",
       "4     This movie made me so angry!! Here I am thinki...  negative\n",
       "...                                                 ...       ...\n",
       "9995  SILVER CITY (2+ outta 5 stars) As a huge fan o...  negative\n",
       "9996  Moscow Zero stole my money and I want it back!...  negative\n",
       "9997  This is the only film I've seen that is made b...  negative\n",
       "9998  This is a story about Shin-ae, who moves to Mi...  positive\n",
       "9999  Leon Errol handles his double role of Uncle Ma...  negative\n",
       "\n",
       "[9987 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop_duplicates()\n",
    "df_inf.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01911227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 40000 entries, 0 to 39999\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   review     40000 non-null  object\n",
      " 1   sentiment  40000 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 625.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df['sentiment'] = df['sentiment'].map({'positive': 1, 'negative': 0})\n",
    "df_inf['sentiment'] = df_inf['sentiment'].map({'positive': 1, 'negative': 0})\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d969196e",
   "metadata": {},
   "source": [
    "##### Length of review distribution (most reviews below 2000 characters, but some as long as 14000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6101d3e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['review'].str.len().hist(bins=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3ef3ff",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f56c75b",
   "metadata": {},
   "source": [
    "##### All text to lowercase and removing all (punct, numb) except a-z and spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f8bbd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review_az'] = df[\"review\"].str.lower().str.replace(r'[^a-z\\s]', ' ', regex=True).str.replace(' +', ' ', regex=True)\n",
    "df_inf['review_az'] = df_inf[\"review\"].str.lower().str.replace(r'[^a-z\\s]', ' ', regex=True).str.replace(' +', ' ', regex=True)\n",
    "\n",
    "#i do it this way because sometimes space after punctuation is missing, so in order to eliminate situation when 'should,but' becomes 'shouldbut'\n",
    "#i replace all that is not letter with space character and then replacing multiple spaces to one space\n",
    "\n",
    "#also stopwords list has forms as \"can't\", but doesn't have form \"cant\", but giving my approach i achieve \"can t\", \n",
    "# so stopwords will filter word \"can\" and i can remove all one letter words resulting in smaller and cleaner data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b1a9a16a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review_az</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I caught this little gem totally by accident b...</td>\n",
       "      <td>1</td>\n",
       "      <td>i caught this little gem totally by accident b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I can't believe that I let myself into this mo...</td>\n",
       "      <td>0</td>\n",
       "      <td>i can t believe that i let myself into this mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>*spoiler alert!* it just gets to me the nerve ...</td>\n",
       "      <td>0</td>\n",
       "      <td>spoiler alert it just gets to me the nerve so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>If there's one thing I've learnt from watching...</td>\n",
       "      <td>0</td>\n",
       "      <td>if there s one thing i ve learnt from watching...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I remember when this was in theaters, reviews ...</td>\n",
       "      <td>0</td>\n",
       "      <td>i remember when this was in theaters reviews s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment  \\\n",
       "0  I caught this little gem totally by accident b...          1   \n",
       "1  I can't believe that I let myself into this mo...          0   \n",
       "2  *spoiler alert!* it just gets to me the nerve ...          0   \n",
       "3  If there's one thing I've learnt from watching...          0   \n",
       "4  I remember when this was in theaters, reviews ...          0   \n",
       "\n",
       "                                           review_az  \n",
       "0  i caught this little gem totally by accident b...  \n",
       "1  i can t believe that i let myself into this mo...  \n",
       "2   spoiler alert it just gets to me the nerve so...  \n",
       "3  if there s one thing i ve learnt from watching...  \n",
       "4  i remember when this was in theaters reviews s...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06314cd",
   "metadata": {},
   "source": [
    "##### Tokenize and remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8898f514",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0c335738",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text):\n",
    "    #tokenize \n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    #filter\n",
    "    clean_tokens = [word for word in tokens if word not in stop_words and len(word) > 1]    \n",
    "    return clean_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "202c9805",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review_az</th>\n",
       "      <th>review_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I caught this little gem totally by accident b...</td>\n",
       "      <td>1</td>\n",
       "      <td>i caught this little gem totally by accident b...</td>\n",
       "      <td>[caught, little, gem, totally, accident, back,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I can't believe that I let myself into this mo...</td>\n",
       "      <td>0</td>\n",
       "      <td>i can t believe that i let myself into this mo...</td>\n",
       "      <td>[believe, let, movie, accomplish, favor, frien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>*spoiler alert!* it just gets to me the nerve ...</td>\n",
       "      <td>0</td>\n",
       "      <td>spoiler alert it just gets to me the nerve so...</td>\n",
       "      <td>[spoiler, alert, gets, nerve, people, remake, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>If there's one thing I've learnt from watching...</td>\n",
       "      <td>0</td>\n",
       "      <td>if there s one thing i ve learnt from watching...</td>\n",
       "      <td>[one, thing, learnt, watching, george, romero,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I remember when this was in theaters, reviews ...</td>\n",
       "      <td>0</td>\n",
       "      <td>i remember when this was in theaters reviews s...</td>\n",
       "      <td>[remember, theaters, reviews, said, horrible, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment  \\\n",
       "0  I caught this little gem totally by accident b...          1   \n",
       "1  I can't believe that I let myself into this mo...          0   \n",
       "2  *spoiler alert!* it just gets to me the nerve ...          0   \n",
       "3  If there's one thing I've learnt from watching...          0   \n",
       "4  I remember when this was in theaters, reviews ...          0   \n",
       "\n",
       "                                           review_az  \\\n",
       "0  i caught this little gem totally by accident b...   \n",
       "1  i can t believe that i let myself into this mo...   \n",
       "2   spoiler alert it just gets to me the nerve so...   \n",
       "3  if there s one thing i ve learnt from watching...   \n",
       "4  i remember when this was in theaters reviews s...   \n",
       "\n",
       "                                       review_tokens  \n",
       "0  [caught, little, gem, totally, accident, back,...  \n",
       "1  [believe, let, movie, accomplish, favor, frien...  \n",
       "2  [spoiler, alert, gets, nerve, people, remake, ...  \n",
       "3  [one, thing, learnt, watching, george, romero,...  \n",
       "4  [remember, theaters, reviews, said, horrible, ...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review_tokens'] = df['review_az'].apply(process_text)\n",
    "df_inf['review_tokens'] = df_inf['review_az'].apply(process_text)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536ee235",
   "metadata": {},
   "source": [
    "##### Stemmer vs Lemmatizer comparison on first review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e54963a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original</th>\n",
       "      <th>Porter Stem</th>\n",
       "      <th>Lemma (Noun)</th>\n",
       "      <th>Lemma (Verb)</th>\n",
       "      <th>lemma_adj</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>caught</td>\n",
       "      <td>caught</td>\n",
       "      <td>caught</td>\n",
       "      <td>catch</td>\n",
       "      <td>caught</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>little</td>\n",
       "      <td>littl</td>\n",
       "      <td>little</td>\n",
       "      <td>little</td>\n",
       "      <td>little</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gem</td>\n",
       "      <td>gem</td>\n",
       "      <td>gem</td>\n",
       "      <td>gem</td>\n",
       "      <td>gem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>totally</td>\n",
       "      <td>total</td>\n",
       "      <td>totally</td>\n",
       "      <td>totally</td>\n",
       "      <td>totally</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>accident</td>\n",
       "      <td>accid</td>\n",
       "      <td>accident</td>\n",
       "      <td>accident</td>\n",
       "      <td>accident</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>dialogue</td>\n",
       "      <td>dialogu</td>\n",
       "      <td>dialogue</td>\n",
       "      <td>dialogue</td>\n",
       "      <td>dialogue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>really</td>\n",
       "      <td>realli</td>\n",
       "      <td>really</td>\n",
       "      <td>really</td>\n",
       "      <td>really</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>works</td>\n",
       "      <td>work</td>\n",
       "      <td>work</td>\n",
       "      <td>work</td>\n",
       "      <td>works</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>audience</td>\n",
       "      <td>audienc</td>\n",
       "      <td>audience</td>\n",
       "      <td>audience</td>\n",
       "      <td>audience</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>definite</td>\n",
       "      <td>definit</td>\n",
       "      <td>definite</td>\n",
       "      <td>definite</td>\n",
       "      <td>definite</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>88 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Original Porter Stem Lemma (Noun) Lemma (Verb) lemma_adj\n",
       "0     caught      caught       caught        catch    caught\n",
       "1     little       littl       little       little    little\n",
       "2        gem         gem          gem          gem       gem\n",
       "3    totally       total      totally      totally   totally\n",
       "4   accident       accid     accident     accident  accident\n",
       "..       ...         ...          ...          ...       ...\n",
       "83  dialogue     dialogu     dialogue     dialogue  dialogue\n",
       "84    really      realli       really       really    really\n",
       "85     works        work         work         work     works\n",
       "86  audience     audienc     audience     audience  audience\n",
       "87  definite     definit     definite     definite  definite\n",
       "\n",
       "[88 rows x 5 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "tokens = df['review_tokens'].iloc[0]\n",
    "\n",
    "comparison_data = []\n",
    "\n",
    "for token in tokens:\n",
    "    stem = stemmer.stem(token)\n",
    "    \n",
    "    lemma_noun = lemmatizer.lemmatize(token, pos='n')\n",
    "    lemma_verb = lemmatizer.lemmatize(token, pos='v') \n",
    "    lemma_adj  = lemmatizer.lemmatize(token, pos='a')\n",
    "\n",
    "    comparison_data.append({\n",
    "        \"Original\": token,\n",
    "        \"Porter Stem\": stem,\n",
    "        \"Lemma (Noun)\": lemma_noun,\n",
    "        \"Lemma (Verb)\": lemma_verb,\n",
    "        \"lemma_adj\": lemma_adj\n",
    "    })\n",
    "\n",
    "pd.DataFrame(comparison_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228fa18f",
   "metadata": {},
   "source": [
    "So above is evident that indeed Stemmer could distort some words in a way that it creates not existing words, by simply chopping end of word_tokenize\n",
    "\n",
    "But lemmatizer has another problem in terms of quality it performs much better, but in order to find and replace correct lemma it needs to know POS - part of speech"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8874d4",
   "metadata": {},
   "source": [
    "##### Lemmatizer implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9b738399",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN  # Default to Noun if uncertain\n",
    "\n",
    "def lemmatize_token_list(tokens):\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    \n",
    "    lemmatized_tokens = []\n",
    "    \n",
    "    for word, tag in pos_tags:\n",
    "        w_tag = get_wordnet_pos(tag)\n",
    "        \n",
    "        lemma = lemmatizer.lemmatize(word, pos=w_tag)\n",
    "        lemmatized_tokens.append(lemma)\n",
    "        \n",
    "    return lemmatized_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf996ba",
   "metadata": {},
   "source": [
    "But It could be very slow, especially on big amounts of data (so this could be viewed as bottleneck in our pipeline, should discover ways of optimizing, paralelizing it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1cc42b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review_lemmatized'] = df['review_tokens'].apply(lemmatize_token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "11b8144f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review_az</th>\n",
       "      <th>review_tokens</th>\n",
       "      <th>review_lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I caught this little gem totally by accident b...</td>\n",
       "      <td>1</td>\n",
       "      <td>i caught this little gem totally by accident b...</td>\n",
       "      <td>[caught, little, gem, totally, accident, back,...</td>\n",
       "      <td>[catch, little, gem, totally, accident, back, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I can't believe that I let myself into this mo...</td>\n",
       "      <td>0</td>\n",
       "      <td>i can t believe that i let myself into this mo...</td>\n",
       "      <td>[believe, let, movie, accomplish, favor, frien...</td>\n",
       "      <td>[believe, let, movie, accomplish, favor, frien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>*spoiler alert!* it just gets to me the nerve ...</td>\n",
       "      <td>0</td>\n",
       "      <td>spoiler alert it just gets to me the nerve so...</td>\n",
       "      <td>[spoiler, alert, gets, nerve, people, remake, ...</td>\n",
       "      <td>[spoiler, alert, get, nerve, people, remake, u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>If there's one thing I've learnt from watching...</td>\n",
       "      <td>0</td>\n",
       "      <td>if there s one thing i ve learnt from watching...</td>\n",
       "      <td>[one, thing, learnt, watching, george, romero,...</td>\n",
       "      <td>[one, thing, learn, watch, george, romero, cre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I remember when this was in theaters, reviews ...</td>\n",
       "      <td>0</td>\n",
       "      <td>i remember when this was in theaters reviews s...</td>\n",
       "      <td>[remember, theaters, reviews, said, horrible, ...</td>\n",
       "      <td>[remember, theater, review, say, horrible, wel...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment  \\\n",
       "0  I caught this little gem totally by accident b...          1   \n",
       "1  I can't believe that I let myself into this mo...          0   \n",
       "2  *spoiler alert!* it just gets to me the nerve ...          0   \n",
       "3  If there's one thing I've learnt from watching...          0   \n",
       "4  I remember when this was in theaters, reviews ...          0   \n",
       "\n",
       "                                           review_az  \\\n",
       "0  i caught this little gem totally by accident b...   \n",
       "1  i can t believe that i let myself into this mo...   \n",
       "2   spoiler alert it just gets to me the nerve so...   \n",
       "3  if there s one thing i ve learnt from watching...   \n",
       "4  i remember when this was in theaters reviews s...   \n",
       "\n",
       "                                       review_tokens  \\\n",
       "0  [caught, little, gem, totally, accident, back,...   \n",
       "1  [believe, let, movie, accomplish, favor, frien...   \n",
       "2  [spoiler, alert, gets, nerve, people, remake, ...   \n",
       "3  [one, thing, learnt, watching, george, romero,...   \n",
       "4  [remember, theaters, reviews, said, horrible, ...   \n",
       "\n",
       "                                   review_lemmatized  \n",
       "0  [catch, little, gem, totally, accident, back, ...  \n",
       "1  [believe, let, movie, accomplish, favor, frien...  \n",
       "2  [spoiler, alert, get, nerve, people, remake, u...  \n",
       "3  [one, thing, learn, watch, george, romero, cre...  \n",
       "4  [remember, theater, review, say, horrible, wel...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d401653",
   "metadata": {},
   "source": [
    "##### Let's extract prepared text data and target, split and vectorize with tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "54aafa1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['review_lemmatized'] \n",
    "y = df['sentiment']\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "057c3ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(\n",
    "    tokenizer=lambda x: x,\n",
    "    preprocessor=lambda x: x,\n",
    "    token_pattern=None\n",
    ")\n",
    "\n",
    "X_train_vec = tfidf.fit_transform(X_train)\n",
    "\n",
    "X_val_vec = tfidf.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7825921f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32000, 73049)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_vec.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee236f52",
   "metadata": {},
   "source": [
    "##### Simple baseline model - LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ebf39567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.882875\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train_vec, y_train)\n",
    "\n",
    "preds = logreg.predict(X_val_vec)\n",
    "print(f\"Accuracy: {accuracy_score(y_val, preds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec8f34d",
   "metadata": {},
   "source": [
    "##### I decided eventualy to try to use stemming on full scale and run logistic regression with it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "35ee6160",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review_az</th>\n",
       "      <th>review_tokens</th>\n",
       "      <th>review_lemmatized</th>\n",
       "      <th>stemmer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I caught this little gem totally by accident b...</td>\n",
       "      <td>1</td>\n",
       "      <td>i caught this little gem totally by accident b...</td>\n",
       "      <td>[caught, little, gem, totally, accident, back,...</td>\n",
       "      <td>[catch, little, gem, totally, accident, back, ...</td>\n",
       "      <td>[caught, littl, gem, total, accid, back, reviv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I can't believe that I let myself into this mo...</td>\n",
       "      <td>0</td>\n",
       "      <td>i can t believe that i let myself into this mo...</td>\n",
       "      <td>[believe, let, movie, accomplish, favor, frien...</td>\n",
       "      <td>[believe, let, movie, accomplish, favor, frien...</td>\n",
       "      <td>[believ, let, movi, accomplish, favor, friend,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>*spoiler alert!* it just gets to me the nerve ...</td>\n",
       "      <td>0</td>\n",
       "      <td>spoiler alert it just gets to me the nerve so...</td>\n",
       "      <td>[spoiler, alert, gets, nerve, people, remake, ...</td>\n",
       "      <td>[spoiler, alert, get, nerve, people, remake, u...</td>\n",
       "      <td>[spoiler, alert, get, nerv, peopl, remak, use,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>If there's one thing I've learnt from watching...</td>\n",
       "      <td>0</td>\n",
       "      <td>if there s one thing i ve learnt from watching...</td>\n",
       "      <td>[one, thing, learnt, watching, george, romero,...</td>\n",
       "      <td>[one, thing, learn, watch, george, romero, cre...</td>\n",
       "      <td>[one, thing, learnt, watch, georg, romero, cre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I remember when this was in theaters, reviews ...</td>\n",
       "      <td>0</td>\n",
       "      <td>i remember when this was in theaters reviews s...</td>\n",
       "      <td>[remember, theaters, reviews, said, horrible, ...</td>\n",
       "      <td>[remember, theater, review, say, horrible, wel...</td>\n",
       "      <td>[rememb, theater, review, said, horribl, well,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment  \\\n",
       "0  I caught this little gem totally by accident b...          1   \n",
       "1  I can't believe that I let myself into this mo...          0   \n",
       "2  *spoiler alert!* it just gets to me the nerve ...          0   \n",
       "3  If there's one thing I've learnt from watching...          0   \n",
       "4  I remember when this was in theaters, reviews ...          0   \n",
       "\n",
       "                                           review_az  \\\n",
       "0  i caught this little gem totally by accident b...   \n",
       "1  i can t believe that i let myself into this mo...   \n",
       "2   spoiler alert it just gets to me the nerve so...   \n",
       "3  if there s one thing i ve learnt from watching...   \n",
       "4  i remember when this was in theaters reviews s...   \n",
       "\n",
       "                                       review_tokens  \\\n",
       "0  [caught, little, gem, totally, accident, back,...   \n",
       "1  [believe, let, movie, accomplish, favor, frien...   \n",
       "2  [spoiler, alert, gets, nerve, people, remake, ...   \n",
       "3  [one, thing, learnt, watching, george, romero,...   \n",
       "4  [remember, theaters, reviews, said, horrible, ...   \n",
       "\n",
       "                                   review_lemmatized  \\\n",
       "0  [catch, little, gem, totally, accident, back, ...   \n",
       "1  [believe, let, movie, accomplish, favor, frien...   \n",
       "2  [spoiler, alert, get, nerve, people, remake, u...   \n",
       "3  [one, thing, learn, watch, george, romero, cre...   \n",
       "4  [remember, theater, review, say, horrible, wel...   \n",
       "\n",
       "                                             stemmer  \n",
       "0  [caught, littl, gem, total, accid, back, reviv...  \n",
       "1  [believ, let, movi, accomplish, favor, friend,...  \n",
       "2  [spoiler, alert, get, nerv, peopl, remak, use,...  \n",
       "3  [one, thing, learnt, watch, georg, romero, cre...  \n",
       "4  [rememb, theater, review, said, horribl, well,...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "\n",
    "df['stemmer'] = df['review_tokens'].apply(lambda x: [stemmer.stem(y) for y in x])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c97127",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['stemmer'] \n",
    "\n",
    "X_train, X_val= train_test_split(\n",
    "    X, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "38ec906f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vec = tfidf.fit_transform(X_train)\n",
    "\n",
    "X_val_vec = tfidf.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e7ae0a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.887875\n"
     ]
    }
   ],
   "source": [
    "logreg.fit(X_train_vec, y_train)\n",
    "\n",
    "preds = logreg.predict(X_val_vec)\n",
    "print(f\"Accuracy: {accuracy_score(y_val, preds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6e719e",
   "metadata": {},
   "source": [
    "##### surprise, surprise stemming is far simpler in code implementation, have much faster runtime, and even by 0.5%, but nevertheless better accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "be369318",
   "metadata": {},
   "outputs": [],
   "source": [
    "cntvect = CountVectorizer(\n",
    "    tokenizer=lambda x: x,\n",
    "    preprocessor=lambda x: x,\n",
    "    token_pattern=None\n",
    ")\n",
    "\n",
    "X_train_cnt = cntvect.fit_transform(X_train)\n",
    "X_val_cnt = cntvect.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6de2a033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.871875\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression(max_iter=300)\n",
    "logreg.fit(X_train_cnt, y_train)\n",
    "\n",
    "preds = logreg.predict(X_val_cnt)\n",
    "print(f\"Accuracy: {accuracy_score(y_val, preds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1d71f9",
   "metadata": {},
   "source": [
    "##### I've tried CountVectorizer, which performs slightly poorer then tf-idf vectorizer, so at the end I will use stemmer and tf-idf vectorizer, since they are giving the best perfomance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a2a1ad",
   "metadata": {},
   "source": [
    "# Modeling neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2c6c7185",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['stemmer'] \n",
    "y = df['sentiment']\n",
    "\n",
    "X_train, X_val= train_test_split(\n",
    "    X, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "    tokenizer=lambda x: x,\n",
    "    preprocessor=lambda x: x,\n",
    "    token_pattern=None,\n",
    "    max_features=10000 #limit data dimmensions for faster computation (from mlflow experiments \n",
    "    #there was no significant difference in accuracy metrics with or without max_feature limmiting)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ba1419fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sparse = tfidf.fit_transform(X_train)\n",
    "X_val_sparse = tfidf.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8e735c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = torch.tensor(X_train_sparse.toarray(), dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "X_val_tensor = torch.tensor(X_val_sparse.toarray(), dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a3e80155",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "baf05d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFIDF_NN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_layers, dropout_rate=0.0):\n",
    "        super(TFIDF_NN, self).__init__()\n",
    "        layers = []\n",
    "        in_dim = input_dim\n",
    "        \n",
    "        for h_dim in hidden_layers:\n",
    "            layers.append(nn.Linear(in_dim, h_dim))    #dynamicaly creating layers for experiments\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout_rate))    #regularization\n",
    "            in_dim = h_dim\n",
    "            \n",
    "        # output layer\n",
    "        layers.append(nn.Linear(in_dim, 1)) \n",
    "        # no sigmoid because will use BCEWithLogitsLoss, which combines sigmoid and bce loss\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb9aded",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_experiment(params):\n",
    "    mlflow.set_experiment(\"TFIDF_Pytorch_Experiment\")\n",
    "    \n",
    "    with mlflow.start_run():\n",
    "        params = params\n",
    "        \n",
    "        mlflow.log_params(params)\n",
    "        \n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model = TFIDF_NN(params[\"input_dim\"], params[\"hidden_dim\"], params[\"dropout\"]).to(device)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=params[\"lr\"])\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "        \n",
    "        for epoch in range(params[\"epochs\"]):\n",
    "            model.train()\n",
    "            epoch_loss = 0\n",
    "            \n",
    "            for inputs, labels in train_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                epoch_loss += loss.item()\n",
    "            \n",
    "            avg_train_loss = epoch_loss / len(train_loader)\n",
    "            print(f\"Epoch {epoch+1} Train Loss: {avg_train_loss:.4f}\")\n",
    "            mlflow.log_metric(\"train_loss\", avg_train_loss, step=epoch)\n",
    "            \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_inputs = X_val_tensor.to(device)\n",
    "            val_labels = y_val_tensor.to(device)\n",
    "            \n",
    "            val_logits = model(val_inputs)\n",
    "            \n",
    "            val_loss = criterion(val_logits, val_labels).item() #val bce loss to detect overfitting\n",
    "            \n",
    "            val_probs = torch.sigmoid(val_logits).cpu().numpy()\n",
    "            y_true = y_val_tensor.cpu().numpy()\n",
    "            \n",
    "            preds = (val_probs > params[\"threshold\"]).astype(int)\n",
    "            \n",
    "            acc = accuracy_score(y_true, preds)\n",
    "            prec = precision_score(y_true, preds, zero_division=0)\n",
    "            rec = recall_score(y_true, preds, zero_division=0)\n",
    "            f1 = f1_score(y_true, preds, zero_division=0)\n",
    "            try:\n",
    "                auc = roc_auc_score(y_true, val_probs)\n",
    "            except ValueError:\n",
    "                auc = 0.5 \n",
    "            \n",
    "            print(f\"Val Loss: {val_loss:.4f} | Acc: {acc:.4f} | AUC: {auc:.4f}\")\n",
    "            print(f\"Precision: {prec:.4f} | Recall: {rec:.4f} | F1: {f1:.4f}\")\n",
    "            \n",
    "            metrics = {\n",
    "                \"val_loss\": val_loss,\n",
    "                \"val_accuracy\": acc,\n",
    "                \"val_precision\": prec,\n",
    "                \"val_recall\": rec,\n",
    "                \"val_f1\": f1,\n",
    "                \"val_roc_auc\": auc\n",
    "            }\n",
    "            mlflow.log_metrics(metrics)\n",
    "            \n",
    "        mlflow.pytorch.log_model(model, name=\"model\")\n",
    "        print(\"Run complete. Check MLflow UI.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b15a5dd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/08 17:55:20 INFO alembic.runtime.plugins: setup plugin alembic.autogenerate.schemas\n",
      "2026/02/08 17:55:20 INFO alembic.runtime.plugins: setup plugin alembic.autogenerate.tables\n",
      "2026/02/08 17:55:20 INFO alembic.runtime.plugins: setup plugin alembic.autogenerate.types\n",
      "2026/02/08 17:55:20 INFO alembic.runtime.plugins: setup plugin alembic.autogenerate.constraints\n",
      "2026/02/08 17:55:20 INFO alembic.runtime.plugins: setup plugin alembic.autogenerate.defaults\n",
      "2026/02/08 17:55:20 INFO alembic.runtime.plugins: setup plugin alembic.autogenerate.comments\n",
      "2026/02/08 17:55:21 INFO alembic.runtime.migration: Context impl SQLiteImpl.\n",
      "2026/02/08 17:55:21 INFO alembic.runtime.migration: Will assume non-transactional DDL.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Train Loss: 0.5591\n",
      "Epoch 2 Train Loss: 0.3784\n",
      "Epoch 3 Train Loss: 0.3339\n",
      "Epoch 4 Train Loss: 0.3041\n",
      "Epoch 5 Train Loss: 0.2774\n",
      "Val Loss: 0.2688 | Acc: 0.8810 | AUC: 0.9588\n",
      "Precision: 0.9223 | Recall: 0.8332 | F1: 0.8755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/08 17:55:29 WARNING mlflow.pytorch: Saving pytorch model by Pickle or CloudPickle format requires exercising caution because these formats rely on Python's object serialization mechanism, which can execute arbitrary code during deserialization.The recommended safe alternative is to set 'export_model' to True to save the pytorch model using the safe graph model format.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run complete. Check MLflow UI.\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"input_dim\": X_train_tensor.shape[1],\n",
    "    \"hidden_dim\": [8,8],\n",
    "    \"dropout\": 0.5,\n",
    "    \"lr\": 0.001,\n",
    "    \"epochs\": 5,\n",
    "    \"batch_size\": 32,\n",
    "    \"threshold\": 0.7\n",
    "}\n",
    "\n",
    "train_experiment(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba4f638",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "- Many different parameters, models configurations (hidden dim list in params) were tried, but I didn't succeed to beat baseline logistic regression, which performs well enough (acc>0.85) and which is also important do it much faster then even simplest neural network\n",
    "\n",
    "- I believe significant improvement could be derived from more complex data preparation\n",
    "\n",
    "- Further steps to improvement could be: carefull stopwords filtering, especialy 'no', 'not' part, which can influence meaning of review\n",
    "\n",
    "- Therefore vectorization technique wich will preserve word order in sentence should be used, instead of 'bagofwords' approach, which do not save such information, and in our sentiment analysis problem it could be segnificant\n",
    "\n",
    "- Advanced techniques as word embeddings could help here\n",
    "\n",
    "- Consequently lstm model, or transformer + embeddings could yield better results, than more basic approaches that was used in this work"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Epam env python 3.11",
   "language": "python",
   "name": "epam"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
